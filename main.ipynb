{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8_LyzKCGL6z-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from os import listdir\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling frequency [Hz]\n",
    "Fs = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutoff frequency [Hz]\n",
    "cutoff = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TEMPLATE = (\n",
    "    'Bayesian classifier:     {bayes_train:.3f}    {bayes_test:.3f}\\n'\n",
    "    'kNN classifier:          {knn_train:.3f}    {knn_test:.3f}\\n'\n",
    "    'Rand forest classifier:  {rf_train:.3f}    {rf_test:.3f}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions (may not all be needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sampling rate (rounded to nearest integer) based on recorded data\n",
    "# [IN]\n",
    "#  x: Series containing the relative time values (from 0-##.##)\n",
    "# [OUT]\n",
    "#  Fs: samples per second, Hz \n",
    "def get_sampling_rate(x):\n",
    "    return round(len(x.index) / x.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows plot of total acceleration values\n",
    "# [IN]\n",
    "#  df: Dataframe containing x, y, z, total acceleration, and time (from 0 to ## seconds)\n",
    "def plot_accel(df):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title('Total Acceleration')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Acceleration [m/s^2]')\n",
    "    plt.plot(df['time'].values, df['atotal'].values, 'b-', linewidth=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply butterworth filter to values in Dataframe; use with df.apply()\n",
    "# [IN]\n",
    "#  df: Dataframe containing x-,y-,z- acceleration values ('atotal' not needed, can be calculated after)\n",
    "def butterworth_lowpass(df):\n",
    "    nyq = 0.5 * Fs\n",
    "    normalized_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(3, normalized_cutoff, btype='lowpass')\n",
    "    return signal.filtfilt(b, a, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows FFT of the total acceleration\n",
    "# [IN]\n",
    "#  df: Dataframe containing acceleration values, must have 'atotal'\n",
    "def plot_fft(df):\n",
    "    w = np.fft.fft(df['atotal'])\n",
    "    freqs = np.fft.fftfreq(len(df['atotal']))\n",
    "    freqs = freqs * Fs\n",
    "\n",
    "    n_samples = len(w)\n",
    "    middle = -1\n",
    "    if (n_samples % 2 == 0):\n",
    "        middle = (n_samples // 2) - 1\n",
    "    else:\n",
    "        middle = (n_samples // 2)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title('One-sided Frequency Spectrum (0 Hz excluded)')\n",
    "    plt.xlabel('Frequency [Hz]')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.plot(freqs[1:middle+1], np.abs(w[1:middle+1]))\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title('Frequency Spectrum')\n",
    "    plt.xlabel('Frequency [Hz]')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.plot(freqs, np.abs(w))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the FFT of the total acceleration into a Dataframe\n",
    "# [IN]\n",
    "#  df: Dataframe containing acceleration values, must have 'atotal'\n",
    "# [OUT]\n",
    "#  fft_df: Dataframe with two columns ['freq', 'value'], containing the result of applying FFT to total acceleration\n",
    "def get_fft(df):\n",
    "    w = np.fft.fft(df['atotal'])\n",
    "    freqs = np.fft.fftfreq(len(df['atotal']))\n",
    "    freqs = freqs * Fs\n",
    "    \n",
    "    fft_df = pd.DataFrame({\n",
    "        'freq': freqs,\n",
    "        'value': np.abs(w)\n",
    "    })\n",
    "    \n",
    "    return fft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formats column names to be ['ax', 'ay', 'az', 'atotal']; Alfred's phone recorded it with units, and 'aT' label for last column\n",
    "# [IN]\n",
    "#  col_name: the column name as a string\n",
    "# [OUT]\n",
    "#  new_col: properly formatted column name (to conform to other group members' data)\n",
    "def format_column(col_name):\n",
    "    new_col = col_name.split('(')[0].strip()\n",
    "    if (new_col == 'aT'):\n",
    "        new_col = 'atotal'\n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For filename with <id>_<pos>_<step_count>.csv format, returns the step count as an integer\n",
    "# [IN]\n",
    "#  filename: string with the filename of input csv\n",
    "# [OUT]\n",
    "#  int(count): the actual step count as an integer\n",
    "def extract_step_count(filename):\n",
    "    if (len(filename.split('_')) < 2):\n",
    "        return np.nan\n",
    "    count_with_csv = filename.split('_')[1]\n",
    "    count = count_with_csv.split('.')[0]\n",
    "    return int(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints predicted step frequencies and step count, and compares with real step count if available\n",
    "# [IN]\n",
    "#  pred_freq: the predicted step frequency\n",
    "#  duration: the entire time taken for data recording\n",
    "#  real_step_count: the actual step count from data collection (default -1 if not available)\n",
    "def print_step_comparisons(pred_freq, duration, real_step_count=-1):\n",
    "    predicted_step_count = pred_freq * duration\n",
    "    predicted_step_60 = pred_freq * 60\n",
    "    \n",
    "    print('Predicted step frequency: ', pred_freq)\n",
    "    print('Predicted step frequency (assuming 60s walking): ', round(predicted_step_60))\n",
    "    print('Predicted step count (using total duration): ', round(predicted_step_count))\n",
    "    if real_step_count > 0:\n",
    "        print('Real step count: ', real_step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns predicted step counts with given step frequency and duration\n",
    "# [IN]\n",
    "#  pred_freq: the predicted step frequency\n",
    "#  duration: the entire time taken for data recording\n",
    "# [OUT]\n",
    "#  predicted_step_count: use given frequency to calculate steps throughout entire duration\n",
    "#  predicted_step_60: use given frequency to calculate steps throught 60 seconds\n",
    "def get_predicted_steps(pred_freq, duration):\n",
    "    predicted_step_count = round(pred_freq * duration)\n",
    "    predicted_step_60 = round(pred_freq * 60)\n",
    "    \n",
    "#     return (predicted_step_count, predicted_step_60)\n",
    "    return predicted_step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data):\n",
    "    df = pd.read_csv(data)\n",
    "    # Filtering dataset\n",
    "    df = df[df.time != 'Time'] # there were 'time' values in the Time column\n",
    "    df = df.drop([0]) # OPTIONAL: drop the first value because there is a gap between the starting time and the subsequent time\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Convert date into a DateTime object\n",
    "    # https://stackoverflow.com/questions/38110263/in-pandas-how-to-convert-a-string-to-a-datetime-object-with-milliseconds\n",
    "    df['time'] = pd.to_datetime(df['time'], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_freqs_with_count(INPUT_DIRECTORY):\n",
    "    results = pd.DataFrame(columns=['filename', 'real_steps', 'freq1', 'steps1', 'freq2','steps2', 'freq3', 'steps3'])\n",
    "\n",
    "    input_list = listdir(INPUT_DIRECTORY)\n",
    "\n",
    "    for f in input_list:\n",
    "        if '.csv' not in f.lower():\n",
    "            continue\n",
    "\n",
    "        new_row = []\n",
    "\n",
    "        new_row.append(f)\n",
    "\n",
    "        real_step_count = extract_step_count(f)\n",
    "        new_row.append(real_step_count)\n",
    "\n",
    "        global Fs  # 'global' keyword needed for assigning value\n",
    "        data = pd.read_csv(INPUT_DIRECTORY + f)\n",
    "        Fs = get_sampling_rate(data['time'])\n",
    "        total_duration = data['time'].iloc[-1]\n",
    "        \n",
    "        data = data.dropna(axis=1, how='all')\n",
    "        data['time'] = data['time'] - data['time'].iloc[0]\n",
    "        data.rename(format_column, axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "        # Filtering data\n",
    "        data_accel = data[['ax','ay','az']].copy()\n",
    "        data_accel_filtered = data_accel.apply(butterworth_lowpass, axis=0)\n",
    "        data_accel_filtered['atotal'] = np.sqrt(data_accel_filtered['ax']**2 + data_accel_filtered['ay']**2 + data_accel_filtered['az']**2)\n",
    "        data_accel_filtered['time'] = data['time'].copy()\n",
    "\n",
    "        \n",
    "        # Look for step frequency\n",
    "        accel_fft = get_fft(data_accel_filtered)\n",
    "        candidate_freqs = accel_fft[accel_fft.freq > 0].nlargest(10, ['value'])\n",
    "\n",
    "        # Method 1:\n",
    "        # Take the candidate frequency with largest magnitude as the step frequency\n",
    "\n",
    "        estimated_freq = candidate_freqs['freq'].iloc[0]\n",
    "        new_row.append(estimated_freq)\n",
    "        new_row.append(get_predicted_steps(estimated_freq, total_duration))\n",
    "\n",
    "        # Method 2:\n",
    "        # Take the mean of the candidate frequencies as step frequency\n",
    "\n",
    "        estimated_freq = candidate_freqs['freq'].mean()\n",
    "        new_row.append(estimated_freq)\n",
    "        new_row.append(get_predicted_steps(estimated_freq, total_duration))\n",
    "\n",
    "        # Method 3:\n",
    "        # Take the mean of candidate frequencies whose magnitudes are greater than half of the maximum magnitude (excluding 0 Hz)\n",
    "\n",
    "        max_value = candidate_freqs['value'].iloc[0]\n",
    "        best_freq = candidate_freqs[candidate_freqs['value'] > (max_value / 2)]\n",
    "\n",
    "        estimated_freq = best_freq['freq'].mean()\n",
    "        new_row.append(estimated_freq)\n",
    "        new_row.append(get_predicted_steps(estimated_freq, total_duration))\n",
    "\n",
    "        results.loc[len(results)] = new_row\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_freqs(INPUT_DIRECTORY):\n",
    "    results = pd.DataFrame(columns=['filename', 'freq1', 'freq2', 'freq3'])\n",
    "\n",
    "    input_list = listdir(INPUT_DIRECTORY)\n",
    "\n",
    "    for f in input_list:\n",
    "        if '.csv' not in f.lower():\n",
    "            continue\n",
    "\n",
    "        new_row = []\n",
    "\n",
    "        new_row.append(f)\n",
    "\n",
    "        total_duration = 0\n",
    "        global Fs  # 'global' keyword needed for assigning value\n",
    "        Fs = 98\n",
    "        data = read_data(INPUT_DIRECTORY + f)\n",
    "        \n",
    "        data = data.dropna(axis=1, how='all')\n",
    "        data['time'] = data['time'] - data['time'].iloc[0]\n",
    "        data.rename(format_column, axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "        # Filtering data\n",
    "        data_accel = data[['ax','ay','az']].copy()\n",
    "        data_accel_filtered = data_accel.apply(butterworth_lowpass, axis=0)\n",
    "        data_accel_filtered['atotal'] = np.sqrt(data_accel_filtered['ax']**2 + data_accel_filtered['ay']**2 + data_accel_filtered['az']**2)\n",
    "        data_accel_filtered['time'] = data['time'].copy()\n",
    "\n",
    "        \n",
    "        # Look for step frequency\n",
    "        accel_fft = get_fft(data_accel_filtered)\n",
    "        candidate_freqs = accel_fft[accel_fft.freq > 0].nlargest(10, ['value'])\n",
    "\n",
    "        # Method 1:\n",
    "        # Take the candidate frequency with largest magnitude as the step frequency\n",
    "\n",
    "        estimated_freq = candidate_freqs['freq'].iloc[0]\n",
    "        new_row.append(estimated_freq)\n",
    "\n",
    "        # Method 2:\n",
    "        # Take the mean of the candidate frequencies as step frequency\n",
    "\n",
    "        estimated_freq = candidate_freqs['freq'].mean()\n",
    "        new_row.append(estimated_freq)\n",
    "\n",
    "        # Method 3:\n",
    "        # Take the mean of candidate frequencies whose magnitudes are greater than half of the maximum magnitude (excluding 0 Hz)\n",
    "\n",
    "        max_value = candidate_freqs['value'].iloc[0]\n",
    "        best_freq = candidate_freqs[candidate_freqs['value'] > (max_value / 2)]\n",
    "\n",
    "        estimated_freq = best_freq['freq'].mean()\n",
    "        new_row.append(estimated_freq)\n",
    "\n",
    "        results.loc[len(results)] = new_row\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity_and_distance(INPUT_DIRECTORY):\n",
    "    # Returns the average speed (m/s) and the total distance walked (m) from the dataset\n",
    "    # [IN]\n",
    "    # df: dataframe\n",
    "    # [OUT]\n",
    "    # 2 variables: speed (the average speed (m/s) and the total distance walked (m)\n",
    "    \n",
    "    results = pd.DataFrame(columns=['filename', 'speed', 'distance'])\n",
    "\n",
    "    input_list = listdir(INPUT_DIRECTORY)\n",
    "\n",
    "    for f in input_list:\n",
    "        if '.csv' not in f.lower():\n",
    "            continue\n",
    "\n",
    "        new_row = []\n",
    "\n",
    "        new_row.append(f)\n",
    "        \n",
    "        global Fs  # 'global' keyword needed for assigning value\n",
    "        \n",
    "        \n",
    "        if (INPUT_DIRECTORY == \"alfred_data/\"):\n",
    "            data = pd.read_csv(INPUT_DIRECTORY + f)\n",
    "            Fs = get_sampling_rate(data['time'])\n",
    "            total_duration = data['time'].iloc[-1]\n",
    "        else:\n",
    "            total_duration = 0\n",
    "            Fs = 98\n",
    "            data = read_data(INPUT_DIRECTORY + f)\n",
    "        \n",
    "            a = np.array([], dtype=np.float64)\n",
    "            for i in data.index:\n",
    "                epoch = data['time'][i].timestamp()\n",
    "                a = np.append(a, epoch)\n",
    "\n",
    "            df_a=pd.DataFrame({'time': a})\n",
    "\n",
    "            data['time'] =  df_a.iloc[1:, 0] - df_a.iat[0, 0]\n",
    "\n",
    "        # Changing time[0] = 0 and drop any rows larger than 60 seconds\n",
    "        data['time'][0] = 0\n",
    "        data = data[data.time <= 60]\n",
    "\n",
    "        data = data.dropna(axis=1, how='all')\n",
    "        data.rename(format_column, axis=1, inplace=True)\n",
    "        \n",
    "        # Filtering data\n",
    "        data_accel = data[['ax','ay','az']].copy()\n",
    "        data_accel_filtered = data_accel.apply(butterworth_lowpass, axis=0)\n",
    "        data_accel_filtered['atotal'] = np.sqrt(data_accel_filtered['ax']**2 + data_accel_filtered['ay']**2 + data_accel_filtered['az']**2)\n",
    "        data_accel_filtered['time'] = data['time'].copy()\n",
    "        data_accel_filtered = data_accel_filtered[['time', 'ax', 'ay', 'az', 'atotal']]\n",
    "\n",
    "        # Velocity: Δv = a⋅Δt \n",
    "        # Displacement: Δp = v⋅Δt.\n",
    "        data_accel_filtered_shift = data_accel_filtered.shift(1)\n",
    "        time_diff = data_accel_filtered['time'] - data_accel_filtered_shift['time']\n",
    "        vx_diff = time_diff * data_accel_filtered['ax']\n",
    "        \n",
    "        temp = pd.DataFrame({'vx_diff': vx_diff})\n",
    "        temp['vx_diff'][0] = 0\n",
    "        data = data_accel_filtered.join(temp)\n",
    "\n",
    "        # data['vx'] = data['vy'] = data['vz'] = data['vtotal'] = 0\n",
    "        data['vx'] = 0\n",
    "\n",
    "        # Calculate final velocity: Δv + initial velocity\n",
    "        for i in data.index:\n",
    "            data.loc[i, 'vx'] = data.iloc[i]['vx_diff'] + data.iloc[i-1]['vx']\n",
    "\n",
    "        # Calculate distance walked: v⋅Δt\n",
    "        data['distance_walked'] = data['vx'] * time_diff\n",
    "\n",
    "        speed = data['vx'].mean()\n",
    "        distance = data['distance_walked'].sum()\n",
    "        \n",
    "        new_row.append(speed)\n",
    "        new_row.append(distance)\n",
    "        \n",
    "        results.loc[len(results)] = new_row\n",
    "        \n",
    "    return(results)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_summary(INPUT_DIRECTORY):\n",
    "    results = pd.DataFrame(columns=['filename','freq1','freq2','freq3','velocity', 'distance','name','phone_position'])\n",
    "\n",
    "    input_list = listdir(INPUT_DIRECTORY)\n",
    "    \n",
    "    name = INPUT_DIRECTORY.split('_')[0].upper()\n",
    "    \n",
    "    for f in input_list:\n",
    "        if '.csv' not in f.lower():\n",
    "            continue\n",
    "\n",
    "        new_row = []\n",
    "\n",
    "        new_row.append(f)\n",
    "\n",
    "        global Fs  # 'global' keyword needed for assigning value\n",
    "        \n",
    "        data = ''\n",
    "        if (INPUT_DIRECTORY == \"alfred_data/\"):\n",
    "            data = pd.read_csv(INPUT_DIRECTORY + f)\n",
    "            Fs = get_sampling_rate(data['time'])\n",
    "            data.rename(format_column, axis=1, inplace=True)\n",
    "        else:\n",
    "            Fs = 98\n",
    "            data = read_data(INPUT_DIRECTORY + f)\n",
    "        \n",
    "            a = np.array([], dtype=np.float64)\n",
    "            for i in data.index:\n",
    "                epoch = data['time'][i].timestamp()\n",
    "                a = np.append(a, epoch)\n",
    "\n",
    "            df_a=pd.DataFrame({'time': a})\n",
    "\n",
    "            data['time'] =  df_a.iloc[1:, 0] - df_a.iat[0, 0]\n",
    "            \n",
    "        \n",
    "        # Use for velocity (something wrong when using original)\n",
    "        data2 = data.copy()\n",
    "        \n",
    "        data = data.dropna(axis=1, how='all')\n",
    "        data['time'] = data['time'] - data['time'].iloc[0]\n",
    "\n",
    "        \n",
    "        \n",
    "        # Filtering data\n",
    "        data_accel = data[['ax','ay','az']].copy()\n",
    "        data_accel_filtered = data_accel.apply(butterworth_lowpass, axis=0)\n",
    "        data_accel_filtered['atotal'] = np.sqrt(data_accel_filtered['ax']**2 + data_accel_filtered['ay']**2 + data_accel_filtered['az']**2)\n",
    "        data_accel_filtered['time'] = data['time'].copy()\n",
    "\n",
    "        \n",
    "        \n",
    "        # Look for step frequency\n",
    "        accel_fft = get_fft(data_accel_filtered)\n",
    "        candidate_freqs = accel_fft[accel_fft.freq > 0].nlargest(10, ['value'])\n",
    "\n",
    "        # Method 1:\n",
    "        # Take the candidate frequency with largest magnitude as the step frequency\n",
    "        estimated_freq = candidate_freqs['freq'].iloc[0]\n",
    "        new_row.append(estimated_freq)\n",
    "\n",
    "        # Method 2:\n",
    "        # Take the mean of the candidate frequencies as step frequency\n",
    "        estimated_freq = candidate_freqs['freq'].mean()\n",
    "        new_row.append(estimated_freq)\n",
    "\n",
    "        # Method 3:\n",
    "        # Take the mean of candidate frequencies whose magnitudes are greater than half of the maximum magnitude (excluding 0 Hz)\n",
    "        max_value = candidate_freqs['value'].iloc[0]\n",
    "        best_freq = candidate_freqs[candidate_freqs['value'] > (max_value / 2)]\n",
    "        estimated_freq = best_freq['freq'].mean()\n",
    "        new_row.append(estimated_freq)\n",
    "\n",
    "\n",
    "        # -------------------------- #\n",
    "        \n",
    "        \n",
    "        # Changing time[0] = 0 and drop any rows larger than 60 seconds\n",
    "        data2['time'][0] = 0\n",
    "        data2 = data2[data2.time <= 60]\n",
    "\n",
    "        data2 = data2.dropna(axis=1, how='all')\n",
    "        data2.rename(format_column, axis=1, inplace=True)\n",
    "        \n",
    "        # Filtering data\n",
    "        data_accel = data2[['ax','ay','az']].copy()\n",
    "        data_accel_filtered = data_accel.apply(butterworth_lowpass, axis=0)\n",
    "        data_accel_filtered['atotal'] = np.sqrt(data_accel_filtered['ax']**2 + data_accel_filtered['ay']**2 + data_accel_filtered['az']**2)\n",
    "        data_accel_filtered['time'] = data2['time'].copy()\n",
    "        data_accel_filtered = data_accel_filtered[['time', 'ax', 'ay', 'az', 'atotal']]\n",
    "\n",
    "        # Velocity: Δv = a⋅Δt \n",
    "        # Displacement: Δp = v⋅Δt.\n",
    "        data_accel_filtered_shift = data_accel_filtered.shift(1)\n",
    "        time_diff = data_accel_filtered['time'] - data_accel_filtered_shift['time']\n",
    "        vx_diff = time_diff * data_accel_filtered['ax']\n",
    "        \n",
    "        temp = pd.DataFrame({'vx_diff': vx_diff})\n",
    "        temp['vx_diff'][0] = 0\n",
    "        data2 = data_accel_filtered.join(temp)\n",
    "\n",
    "        # data['vx'] = data['vy'] = data['vz'] = data['vtotal'] = 0\n",
    "        data2['vx'] = 0\n",
    "\n",
    "        # Calculate final velocity: Δv + initial velocity\n",
    "        for i in data2.index:\n",
    "            data2.loc[i, 'vx'] = data2.iloc[i]['vx_diff'] + data2.iloc[i-1]['vx']\n",
    "\n",
    "        # Calculate distance walked: v⋅Δt\n",
    "        data2['distance_walked'] = abs(data2['vx'] * time_diff)\n",
    "        data2['vx'] = abs(data2['vx'])\n",
    "\n",
    "        speed = data2['vx'].mean()\n",
    "        distance = data2['distance_walked'].sum()\n",
    "        \n",
    "        new_row.append(speed)\n",
    "        new_row.append(distance)\n",
    "        \n",
    "        new_row.append(name)\n",
    "        \n",
    "        if 'ankle' in f.lower():\n",
    "            new_row.append('ankle')\n",
    "        elif 'hand' in f.lower():\n",
    "            new_row.append('hand')\n",
    "        else:\n",
    "            new_row.append('pocket')\n",
    "        \n",
    "        results.loc[len(results)] = new_row\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(df):\n",
    "    X = df[['freq1', 'freq2', 'freq3']].values\n",
    "    y = df['name'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    bayes_model = GaussianNB()\n",
    "    knn_model = KNeighborsClassifier(n_neighbors= 3)\n",
    "    rf_model = RandomForestClassifier(n_estimators = 100, max_depth = 3)\n",
    "    \n",
    "    models = [bayes_model, knn_model, rf_model]\n",
    "    for i, m in enumerate(models): \n",
    "        m.fit(X_train, y_train)\n",
    "    \n",
    "    print(OUTPUT_TEMPLATE.format(\n",
    "        bayes_train = bayes_model.score(X_train, y_train),\n",
    "        bayes_test = bayes_model.score(X_test, y_test),\n",
    "        knn_train = knn_model.score(X_train, y_train),\n",
    "        knn_test = knn_model.score(X_test, y_test),\n",
    "        rf_train = rf_model.score(X_train, y_train),\n",
    "        rf_test = rf_model.score(X_test, y_test),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>real_steps</th>\n",
       "      <th>freq1</th>\n",
       "      <th>steps1</th>\n",
       "      <th>freq2</th>\n",
       "      <th>steps2</th>\n",
       "      <th>freq3</th>\n",
       "      <th>steps3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ankle1_100.csv</td>\n",
       "      <td>100</td>\n",
       "      <td>0.011661</td>\n",
       "      <td>1</td>\n",
       "      <td>1.315357</td>\n",
       "      <td>113</td>\n",
       "      <td>0.876904</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ankle2_108.csv</td>\n",
       "      <td>108</td>\n",
       "      <td>0.011696</td>\n",
       "      <td>1</td>\n",
       "      <td>1.632796</td>\n",
       "      <td>139</td>\n",
       "      <td>1.148182</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ankle3_112.csv</td>\n",
       "      <td>112</td>\n",
       "      <td>0.011057</td>\n",
       "      <td>1</td>\n",
       "      <td>1.479388</td>\n",
       "      <td>134</td>\n",
       "      <td>1.159112</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ankle4_112.csv</td>\n",
       "      <td>112</td>\n",
       "      <td>0.011193</td>\n",
       "      <td>1</td>\n",
       "      <td>1.289480</td>\n",
       "      <td>115</td>\n",
       "      <td>0.870847</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hand1_114.csv</td>\n",
       "      <td>114</td>\n",
       "      <td>1.758506</td>\n",
       "      <td>126</td>\n",
       "      <td>0.815053</td>\n",
       "      <td>58</td>\n",
       "      <td>1.758506</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hand2_114.csv</td>\n",
       "      <td>114</td>\n",
       "      <td>1.739272</td>\n",
       "      <td>122</td>\n",
       "      <td>1.069225</td>\n",
       "      <td>75</td>\n",
       "      <td>1.739272</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hand3_112.csv</td>\n",
       "      <td>112</td>\n",
       "      <td>1.743913</td>\n",
       "      <td>124</td>\n",
       "      <td>0.983060</td>\n",
       "      <td>70</td>\n",
       "      <td>1.736881</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hand4_111.csv</td>\n",
       "      <td>111</td>\n",
       "      <td>1.713106</td>\n",
       "      <td>130</td>\n",
       "      <td>1.043677</td>\n",
       "      <td>79</td>\n",
       "      <td>1.726283</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pocket1_114.csv</td>\n",
       "      <td>114</td>\n",
       "      <td>0.010644</td>\n",
       "      <td>1</td>\n",
       "      <td>1.167701</td>\n",
       "      <td>110</td>\n",
       "      <td>0.305142</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pocket2_117.csv</td>\n",
       "      <td>117</td>\n",
       "      <td>0.012046</td>\n",
       "      <td>1</td>\n",
       "      <td>1.874362</td>\n",
       "      <td>155</td>\n",
       "      <td>0.368608</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pocket3_117.csv</td>\n",
       "      <td>117</td>\n",
       "      <td>0.012424</td>\n",
       "      <td>1</td>\n",
       "      <td>2.226293</td>\n",
       "      <td>179</td>\n",
       "      <td>1.276072</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pocket4_114.csv</td>\n",
       "      <td>114</td>\n",
       "      <td>0.012461</td>\n",
       "      <td>1</td>\n",
       "      <td>1.262325</td>\n",
       "      <td>101</td>\n",
       "      <td>0.720260</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename  real_steps     freq1  steps1     freq2  steps2     freq3  \\\n",
       "0    ankle1_100.csv         100  0.011661       1  1.315357     113  0.876904   \n",
       "1    ankle2_108.csv         108  0.011696       1  1.632796     139  1.148182   \n",
       "2    ankle3_112.csv         112  0.011057       1  1.479388     134  1.159112   \n",
       "3    ankle4_112.csv         112  0.011193       1  1.289480     115  0.870847   \n",
       "4     hand1_114.csv         114  1.758506     126  0.815053      58  1.758506   \n",
       "5     hand2_114.csv         114  1.739272     122  1.069225      75  1.739272   \n",
       "6     hand3_112.csv         112  1.743913     124  0.983060      70  1.736881   \n",
       "7     hand4_111.csv         111  1.713106     130  1.043677      79  1.726283   \n",
       "8   pocket1_114.csv         114  0.010644       1  1.167701     110  0.305142   \n",
       "9   pocket2_117.csv         117  0.012046       1  1.874362     155  0.368608   \n",
       "10  pocket3_117.csv         117  0.012424       1  2.226293     179  1.276072   \n",
       "11  pocket4_114.csv         114  0.012461       1  1.262325     101  0.720260   \n",
       "\n",
       "    steps3  \n",
       "0       75  \n",
       "1       98  \n",
       "2      105  \n",
       "3       78  \n",
       "4      126  \n",
       "5      122  \n",
       "6      123  \n",
       "7      131  \n",
       "8       29  \n",
       "9       31  \n",
       "10     103  \n",
       "11      58  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIRECTORY = \"alfred_data/\"\n",
    "results = get_step_freqs_with_count(INPUT_DIRECTORY)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIRECTORY = \"huy_data/\"\n",
    "# results = get_step_freqs(INPUT_DIRECTORY)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIRECTORY = \"janit_data/\"\n",
    "# results = get_step_freqs(INPUT_DIRECTORY)\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed and distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # took ~25s for alfred_data and ~5s for huy_data/janit_data\n",
    "# INPUT_DIRECTORY = \"alfred_data/\"\n",
    "# results = get_velocity_and_distance(INPUT_DIRECTORY)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIRECTORY = \"huy_data/\"\n",
    "# results = get_velocity_and_distance(INPUT_DIRECTORY)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIRECTORY = \"janit_data/\"\n",
    "# results = get_velocity_and_distance(INPUT_DIRECTORY)\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'vx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'vx'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m temp1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malfred_data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m temp2 \u001b[38;5;241m=\u001b[39m get_data_summary(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuy_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m temp3 \u001b[38;5;241m=\u001b[39m get_data_summary(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjanit_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36mget_data_summary\u001b[1;34m(INPUT_DIRECTORY)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Calculate distance walked: v⋅Δt\u001b[39;00m\n\u001b[0;32m    110\u001b[0m data2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_walked\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(data2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m time_diff)\n\u001b[1;32m--> 111\u001b[0m data2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    113\u001b[0m speed \u001b[38;5;241m=\u001b[39m data2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    114\u001b[0m distance \u001b[38;5;241m=\u001b[39m data2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_walked\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'vx'"
     ]
    }
   ],
   "source": [
    "temp1 = get_data_summary(\"alfred_data/\")\n",
    "temp2 = get_data_summary(\"huy_data/\")\n",
    "temp3 = get_data_summary(\"janit_data/\")\n",
    "\n",
    "data_summary = pd.concat([temp1, temp2, temp3], ignore_index=True)\n",
    "data_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
